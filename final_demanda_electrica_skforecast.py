# -*- coding: utf-8 -*-
"""FINAL_Demanda_electrica_skforecast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LUAvgP8uoJnAe7oZ9B7cOImIA_4mI1gt
"""

pip install -q skforecast

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import holidays

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.ensemble import RandomForestRegressor
from skforecast.recursive import ForecasterRecursive
from skforecast.model_selection import TimeSeriesFold, grid_search_forecaster, backtesting_forecaster
from skforecast.preprocessing import TimeSeriesDifferentiator
from sklearn.metrics import mean_squared_error, mean_absolute_error

from xgboost import XGBRegressor
from skforecast.recursive import ForecasterRecursive

data = pd.read_excel("DemandaCOES_.xlsx",skiprows=3)

data['FECHA'] = pd.to_datetime(data['FECHA'], format='%d/%m/%Y %H:%M')
data.set_index('FECHA',inplace=True)
data.rename(columns={'EJECUTADO':'Demand'},inplace=True)
data = data.asfreq("30min")
data.head()

data.index

"""## VISUALIZANDO SERIE"""

data.Demand.plot(figsize=(16,6))

"""## ANALISIS DE TENDENCIA"""

data.Demand.plot(figsize=(16,6),label="Consumo Electrico")
data.Demand.rolling(48).mean().plot(label='Media movil (24 horas)')
plt.legend()
plt.show()

"""## ANALISIS DE TENDENCIA SEMANAL"""

data.Demand.plot(figsize=(16,6),label="Consumo Electrico")
data.Demand.rolling(48*7).mean().plot(label='Media movil (7 dias)')
plt.legend()
plt.show()

"""## ANALISIS DE TENDENCIA PRIMERA SEMANA"""

data.Demand[:(48*7)].plot(figsize=(16, 6))

"""## ANALISIS DE TENDENCIA 24 HORAS"""

data.Demand.plot(figsize=(16, 6), label="Consumo Electrico")
data.Demand.rolling(48).mean().plot(label='Media movil ( 24 horas)')
plt.legend()
plt.show()

"""## FEATURING ENGINEERING

## EFECTO CICLICO
"""

data['ciclo'] = data.index.map(lambda t: (t.hour * 60 + t.minute) / (24 * 60))

data[['Demand','ciclo']][-48*7:].plot(figsize=(16,8),subplots=True)
plt.suptitle('Demanda de energía y característica cíclica temporal de la última semana', fontsize=16)
plt.show()

"""## EFECTO DIARIO"""

days_translation = {
    'Monday': '1Lunes',
    'Tuesday': '2Martes',
    'Wednesday': '3Miércoles',
    'Thursday': '4Jueves',
    'Friday': '5Viernes',
    'Saturday': '6Sábado',
    'Sunday': '0Domingo'
}

data['dia'] = data.index.day_name().map(days_translation)

data2 = pd.get_dummies(data,columns=['dia'], dtype=int)

data2.columns

data2.head()

"""## DEMANDA DE ENERGÍA Y ESTADO DEL DÍA DE LA SEMANA PARA LA ÚLTIMA SEMANA"""

data2[['Demand','dia_0Domingo','dia_1Lunes','dia_2Martes','dia_3Miércoles','dia_4Jueves','dia_5Viernes','dia_6Sábado']][-48*7:].plot(figsize=(16,32),subplots=True)
plt.show()

"""## EFECTO FERIADO"""

pe = holidays.Peru(years=[2023, 2024], observed=True)
data2['feriado'] = data2.index.normalize().isin(pe).astype(int)

data2[['Demand','feriado']][-48*7:].plot(figsize=(16,6),subplots=True)
plt.suptitle('Demanda de Energía y día Feriado para la Última Semana', fontsize=16)
plt.show()

"""## VISUALIZACIÓN CONSUMO ENERGÍA PROMEDIO POR HORA"""

data2['hour'] = data2.index.hour
# Agrupa por hora y calcular la demanda media
hourly_demand = data2.groupby('hour')['Demand'].mean()

plt.figure(figsize=(12, 6))
sns.lineplot(x=hourly_demand.index, y=hourly_demand.values, marker='o', color='b')
plt.title('Consumo de Energía Promedio por Hora', fontsize=16)
plt.xlabel('Hora del Día', fontsize=12)
plt.ylabel('Consumo Promedio (MW)', fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.xticks(hourly_demand.index)
plt.tight_layout()
plt.show()

"""## VISUALIZACIÓN CONSUMO DE ENERGÍA PROMEDIO POR DÍA DE LA SEMANA

USANDO DATA2 GRAFICANDO

VISUALIZACIÓN CONSUMO DE ENERGÍA PROMEDIO POR DÍA DE LA SEMANA
"""

# Identificar las nuevas variables ficticias para los días de la semana
day_columns = [col for col in data2.columns if col.startswith('dia_')]

# Calcular la demanda media para cada día de la semana utilizando las columnas ficticias
daily_demand_from_dummies = {}
for col in day_columns:
    # Filtrar el DataFrame donde la columna ficticia es 1 y calcular la media de 'Demanda'
    mean_demand = data2.loc[data2[col] == 1, 'Demand'].mean()
    # Almacenar el resultado con el nombre del día original como clave
    day_name = col.replace('dia_', '')
    daily_demand_from_dummies[day_name] = mean_demand

# Conviertir el diccionario en una serie de pandas y ordenarlo por el prefijo numérico
daily_demand = pd.Series(daily_demand_from_dummies)
daily_demand = daily_demand.sort_index()

# Graficar los datos
plt.figure(figsize=(12, 6))
sns.barplot(x=daily_demand.index, y=daily_demand.values, palette='viridis')
plt.title('Consumo de Energía Promedio por Día de la Semana', fontsize=16)
plt.xlabel('Día de la Semana', fontsize=12)
plt.ylabel('Consumo Promedio (MW)', fontsize=12)
plt.xticks(ticks=np.arange(len(daily_demand.index)), labels=[d[1:] for d in daily_demand.index])
plt.tight_layout()

"""## VISUALIZACIÓN CONSUMO DE ENERGÍA PROMEDIO: DÍAS NORMALES VS. FERIADOS"""

pe = holidays.Peru(years=[2023, 2024], observed=True)
# Columna 'feriado' (1 para días festivos, 0 para días normales)
data2['feriado'] = data2.index.normalize().isin(pe).astype(int)

plt.figure(figsize=(12, 6))
# Agrupa por estado de vacaciones y calcula la demanda media
holiday_demand = data2.groupby('feriado')['Demand'].mean()
sns.barplot(x=holiday_demand.index, y=holiday_demand.values, palette=['skyblue', 'salmon'])
plt.title('Consumo de Energía Promedio: Días Normales vs. Feriados', fontsize=16)
plt.xlabel('Tipo de Día (0:Normal, 1:Feriado)', fontsize=12)
plt.ylabel('Consumo Promedio (MW)', fontsize=12)
plt.xticks(ticks=[0, 1], labels=['Día Normal', 'Feriado'])
plt.tight_layout()
plt.show()

"""# PROYECTO FINAL"""

data = pd.read_excel("DemandaCOES_.xlsx",skiprows=3)

data['FECHA'] = pd.to_datetime(data['FECHA'], format='%d/%m/%Y %H:%M')
data.set_index('FECHA',inplace=True)
data.rename(columns={'EJECUTADO':'Demand'},inplace=True)
data = data.asfreq("30min")
data.head()

"""Feature Engineering (Exogenous Variables)"""

def hora_minuto(hora,minuto):
  result = (hora * 60 + minuto) / (24 * 60)
  return result

data['ciclo'] = data.index.map(lambda t: (t.hour * 60 + t.minute) / (24 * 60))

days_translation = {
    'Monday': '1Lunes',
    'Tuesday': '2Martes',
    'Wednesday': '3Miércoles',
    'Thursday': '4Jueves',
    'Friday': '5Viernes',
    'Saturday': '6Sábado',
    'Sunday': '0Domingo'
}

data['dia'] = data.index.day_name().map(days_translation)
data2 = pd.get_dummies(data,columns=['dia'], dtype=int)

pe = holidays.Peru(years=[2023, 2024], observed=True)
data2['feriado'] = data2.index.normalize().isin(pe).astype(int)

data2.head()

"""Machine Learning Analysis (XGBoost with skforecast)"""

# Crear y entrenar al pronosticador
regressor = XGBRegressor(n_estimators=250, max_depth=8, random_state=123)
forecaster = ForecasterRecursive(
                 regressor = regressor,
                 lags      = 48
             )
forecaster.fit(y=data2.Demand)

# Hacer predicciones
data2.Demand[-(48*7*2):].plot(figsize=(16,4))
forecaster.predict(48).plot()

# Dividir datos para ajustar y probar hiperparámetros
# steps = 48*7*4 # Para separar las últimas 4 semanas
steps = 48*7 # Para separar la última semana
x_train = data2.drop(columns=['Demand'])[:-steps]
x_test  = data2.drop(columns=['Demand'])[-steps:]
y_train = data2['Demand'][:-steps]
y_test  = data2['Demand'][-steps:]

# Configuración de búsqueda de grid search
forecaster = ForecasterRecursive(
                 regressor = XGBRegressor(random_state=123),
                 lags      = 48*7#esto es la memoria del modelo (una semana de datos)
             )

# Pliegues de entrenamiento y validación grid search
# 48*7, # La evaluación se realizará por periodos de 1 semana.
cv = TimeSeriesFold(
      steps              = 48, # La evaluación es por periodos de 1 día.
      initial_train_size = int(len(x_train) - steps), # Iniciar evaluación desde hace una semana
      refit              = False,
      fixed_train_size   = False
    )

# Valores candidatos para rezagos e hiperparámetros
lags_grid = [48,96,48*7]

# ESTOS PARÁMETROS TARDARONS 3 HORAS 20 MINUTOS EN GRID SEARCH FORECASTER
"""# Candidate values for regressor's hyperparameters
param_grid = {
    'n_estimators': [100, 250, 500, 750],
    'max_depth': [3, 5, 8, 10],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.9],
    'colsample_bytree': [0.7, 0.9]
}"""

param_grid = {
    'n_estimators': [250],
    'max_depth': [5, 8, 10],
    'learning_rate': [0.05],
    #'subsample': [0.7, 0.9],
    #'colsample_bytree': [0.9]
}

# Run grid search
results_grid = grid_search_forecaster(
                    forecaster  = forecaster,
                    y           = y_train,
                    cv          = cv,
                    exog        = x_train,
                    param_grid  = param_grid,
                    lags_grid   = lags_grid,
                    metric      = 'mean_squared_error',
                    return_best = True,
                    n_jobs      = 'auto',
                    verbose     = False)

# PARÁMETROS OBTENIDOS
"""
`Forecaster` refitted using the best-found lags and parameters, and the whole data set:
  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48
 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72
 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96]
  Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 250}
  Backtesting metric: 8938.029978205772
  """

# Entrenar el modelo final con los mejores parámetros
regressor = XGBRegressor(
    n_estimators=250,
    max_depth=8,
    learning_rate=0.05,
    #colsample_bytree=0.7,
    #subsample=0.7,
    random_state=123
)
forecaster = ForecasterRecursive(
                 regressor = regressor,
                 lags      = 96
                              )
forecaster.fit(y=y_train,exog=x_train)

# Hacer predicciones con el modelo final
predictions = forecaster.predict(#me va generar un pronostico de una semana hacia el futuro
                steps = steps,#cada 30 minutos hasta llegar a una semana
                exog  = x_test#el modelo va conocer los dias de la semana, la hora
              )

# Graficar los resultados
fig, ax = plt.subplots(figsize=(24, 4))
y_train[-steps:].plot(ax=ax, label='train')
y_test.plot(ax=ax, label='test')
predictions.plot(ax=ax, label='predictions')
ax.legend()
plt.show()

"""EVALUACIÓN"""

# Calcular el error cuadrático medio para el modelo de aprendizaje automático
error_mse1 = mean_squared_error(
                y_true = y_test,
                y_pred = predictions
            )
print(f"Test error 1 (MSE): {error_mse1}")

# Calcular Root Mean Squared Error (RMSE)
error_rmse = np.sqrt(error_mse1)

print(f"Root Mean Squared Error (RMSE): {error_rmse}")

# MAE es el promedio de las diferencias absolutas entre las predicciones y los valores reales.
# Es menos sensible a los valores atípicos que el RMSE.
mae = mean_absolute_error(y_true=y_test, y_pred=predictions)
print(f"Mean Absolute Error (MAE): {mae:.2f} Megawatts")

### 1. Mean Absolute Percentage Error (MAPE)
# Esta es una buena medida de "precisión promedio" en términos porcentuales.
def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    # Filtra los valores actuales que sean cero para evitar errores de división por cero
    non_zero_elements = y_true != 0
    if not np.any(non_zero_elements):
        return np.nan # O 0, dependiendo de cómo quieras manejar todos los ceros

    return np.mean(np.abs((y_true[non_zero_elements] - y_pred[non_zero_elements]) / y_true[non_zero_elements])) * 100

# Calcular MAPE
# Asegurarse de que y_test y las predicciones estén definidas en el alcance de su cuaderno
mape = mean_absolute_percentage_error(y_true=y_test, y_pred=predictions)
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
# 100 - 1.54 = 98.46% Preciso